{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeff4a75-11e0-4591-9042-2af0e9e0d8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# better display of review text in dataframes\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "\n",
    "# Seaborn options\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58faa19c-a0ef-45f1-9526-ba9ef600018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-16.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.9 MB 87 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow) (1.23.3)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-16.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d3309aa-b12a-4849-b718-c13beb99ab31",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (401279685.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install fastparquet\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install fastparquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb12075b-b9d5-4f85-8f1b-85e12c5028e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ex5/.local/lib/python3.8/site-packages (4.40.0)\n",
      "Requirement already satisfied: requests in /home/ex5/.local/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ex5/.local/lib/python3.8/site-packages (from transformers) (2024.4.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ex5/.local/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ex5/.local/lib/python3.8/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: filelock in /home/ex5/.local/lib/python3.8/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ex5/.local/lib/python3.8/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ex5/.local/lib/python3.8/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ex5/.local/lib/python3.8/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ex5/.local/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ex5/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ex5/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506e9687-afb4-4627-9219-3a18957c8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PARQUET_PATH_train = \"train-00000-of-00001.parquet\"\n",
    "\n",
    "PARQUET_PATH_test = \"test-00000-of-00001.parquet\"\n",
    "PARQUET_PATH_validation =\"validation-00000-of-00001.parquet\"\n",
    "\n",
    "train=pd.read_parquet(PARQUET_PATH_train)\n",
    "train=train[:20000]\n",
    "test=pd.read_parquet(PARQUET_PATH_test)\n",
    "test=test[:7500]\n",
    "validation=pd.read_parquet(PARQUET_PATH_validation)\n",
    "validation=validation[:7500]\n",
    "\n",
    "train_reviews=train.review\n",
    "\n",
    "train_labels=train.label\n",
    "\n",
    "test_reviews=test.review\n",
    "test_labels=test.label\n",
    "\n",
    "validation_reviews=validation.review\n",
    "validation_labels=validation.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c627dca-9ce3-4332-b42c-5d27025a28e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560be94a-23d2-4485-93fd-b3497b9d3647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 08:42:46.568797: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-22 08:42:47.033585: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-22 08:42:48.706095: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-22 08:42:48.710277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 08:42:52.048423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f9ecff7-3093-491f-b7ae-c21b323359d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SentencePiece\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: SentencePiece\n",
      "Successfully installed SentencePiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99cd46c3-4350-4dbe-88bf-f0462a188d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69bac07bf1c4b32bfee5cca1e3e251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76514f355843447394be7edc74b4e653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567089cc18c2463e8345eac044addf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64211d9e014040fcbf282ba394815526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CamembertTokenizer\n",
    "\n",
    "model_name = \"camembert-base\"\n",
    "tokenizer = CamembertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78b4ac1-6de8-45de-a2e2-17fc55d31949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Si vous cherchez du cinéma abrutissant à tous les étages,n\\'ayant aucune peur du cliché en castagnettes et moralement douteux,\"From Paris with love\" est fait pour vous.Toutes les productions Besson,via sa filière EuropaCorp ont de quoi faire naître la moquerie.Paris y est encore une fois montrée comme une capitale exotique,mais attention si l\\'on se dirige vers la banlieue,on y trouve tout plein d\\'intégristes musulmans prêts à faire sauter le caisson d\\'une ambassadrice américaine.Nauséeux.Alors on se dit qu\\'on va au moins pouvoir apprécier la déconnade d\\'un classique buddy-movie avec le jeune agent aux dents longues obligé de faire équipe avec un vieux lou complètement timbré.Mais d\\'un côté,on a un Jonathan Rhys-meyers fayot au possible,et de l\\'autre un John Travolta en total délire narcissico-badass,crâne rasé et bouc proéminent à l\\'appui.Sinon,il n\\'y a aucun scénario.Seulement,des poursuites débiles sur l\\'autoroute,Travolta qui étale 10 mecs à l\\'arme blanche en 8 mouvements(!!)ou laisse son associé se faire démolir la tronche pendant qu\\'il scrute à la jumelle.Ca pourrait être un plaisir coupable,tellement c\\'est \"hénaurme\",c\\'est juste de la daube dans la droite lignée d\\'un \"Transporteur\",\"Taken\"ou \"Banlieue 13\".'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "some_review = train_reviews[0]\n",
    "some_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4fedd84-7079-41df-a744-109f9cc1c3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Si',\n",
       " '▁vous',\n",
       " '▁cherchez',\n",
       " '▁du',\n",
       " '▁cinéma',\n",
       " '▁abruti',\n",
       " 'ssant',\n",
       " '▁à',\n",
       " '▁tous',\n",
       " '▁les',\n",
       " '▁étages',\n",
       " ',',\n",
       " 'n',\n",
       " \"'\",\n",
       " 'ayant']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(some_review)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d82cef-92d0-4a28-8637-9e124860bbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 168, 39, 3162, 25, 1545, 29470, 2927, 15, 117, 19, 9339, 7, 255, 11]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(some_review)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec949ef-5b64-46b5-9b25-aa260c1fa721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Si vous cherchez du cinéma abrutissant à tous les étages,n\\'ayant aucune peur du cliché en castagnettes et moralement douteux,\"From Paris with love\" est fait pour vous.Toutes les productions Besson,via sa filière EuropaCorp ont de quoi faire naître la moquerie.Paris y est encore une fois montrée comme une capitale exotique,mais attention si l\\'on se dirige vers la banlieue,on y trouve tout plein d\\'intégristes musulmans prêts à faire sauter le caisson d\\'une ambassadrice américaine.Nauséeux.Alors on se dit qu\\'on va au moins pouvoir apprécier la déconnade d\\'un classique buddy-movie avec le jeune agent aux dents longues obligé de faire équipe avec un vieux lou complètement timbré.Mais d\\'un côté,on a un Jonathan Rhys-meyers fayot au possible,et de l\\'autre un John Travolta en total délire narcissico-badass,crâne rasé et bouc proéminent à l\\'appui.Sinon,il n\\'y a aucun scénario.Seulement,des poursuites débiles sur l\\'autoroute,Travolta qui étale 10 mecs à l\\'arme blanche en 8 mouvements(!!)ou laisse son associé se faire démolir la tronche pendant qu\\'il scrute à la jumelle.Ca pourrait être un plaisir coupable,tellement c\\'est \"hénaurme\",c\\'est juste de la daube dans la droite lignée d\\'un \"Transporteur\",\"Taken\"ou \"Banlieue 13\".</s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(some_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "602089e0-6859-4ee8-8ee4-753995a2b460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 128.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Calculate the length of encoded reviews\n",
    "reviews_len = [len(tokenizer.encode(review, max_length=512)) for review in train_reviews]\n",
    "\n",
    "# Display statistics\n",
    "print(\"Average length: {:.1f}\".format(np.mean(reviews_len)))\n",
    "\n",
    "# Check and create the directory if it does not exist\n",
    "output_dir = 'img/bert/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Save the plot in the specified directory\n",
    "plt.savefig('img/bert/number_of_tokens.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7fa7883-c470-4367-bfe4-e43f87ee9da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea4a13eb-f5f7-4579-8628-fcfe9b024f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582 reviews with LEN > 400 (2.91 % of total data)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = 400 # in terms of generated tokens (not words)\n",
    "\n",
    "short_reviews = sum(np.array(reviews_len) <= MAX_SEQ_LEN)\n",
    "long_reviews = sum(np.array(reviews_len) > MAX_SEQ_LEN)\n",
    "\n",
    "print(\"{} reviews with LEN > {} ({:.2f} % of total data)\".format(\n",
    "    long_reviews,\n",
    "    MAX_SEQ_LEN,\n",
    "    100 * long_reviews / len(reviews_len)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1e7cbf8-e0cf-4a12-8d4c-20dba802489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_reviews(tokenizer, reviews, max_length):\n",
    "    token_ids = np.zeros(shape=(len(reviews), max_length),\n",
    "                         dtype=np.int32)\n",
    "    for i, review in enumerate(reviews):\n",
    "        encoded = tokenizer.encode(review, max_length=max_length)\n",
    "        token_ids[i, 0:len(encoded)] = encoded\n",
    "    attention_mask = (token_ids != 0).astype(np.int32)\n",
    "    return {\"input_ids\": token_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63ad332a-18d5-4a2e-b967-23770ab0a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = encode_reviews(tokenizer, train_reviews, MAX_SEQ_LEN)\n",
    "encoded_valid = encode_reviews(tokenizer, validation_reviews, MAX_SEQ_LEN)\n",
    "encoded_test = encode_reviews(tokenizer, test_reviews, MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b354f057-cf95-410b-b858-76fa9fb3e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(validation_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b92938b-750b-45f8-bf60-269a6095e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CamembertPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tokenizer, max_seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def fit(self, X=None):\n",
    "        pass\n",
    "                                    \n",
    "    def transform(self, X, y):\n",
    "        # 1. Tokenize\n",
    "        X_encoded = encode_reviews(self.tokenizer, X, self.max_seq_length)\n",
    "        # 2. Labels\n",
    "        y_array = np.array(y)\n",
    "        return X_encoded, y_array     \n",
    "                                                                                    \n",
    "    def fit_transform(self, X, y):        \n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5234528-d5b7-47c3-857c-e0fafa4f63a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601fc7e1e8884f8fa9583fcd0c650610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb513ee32164d9fa7ffceaf6512b45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/545M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 08:46:10.412289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-22 08:46:10.412856: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
      "\n",
      "Some layers of TFCamembertForSequenceClassification were not initialized from the model checkpoint at jplu/tf-camembert-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFCamembertForSequenceClassification\n",
    "\n",
    "model = TFCamembertForSequenceClassification.from_pretrained(\"jplu/tf-camembert-base\")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=5e-6, epsilon=1e-08)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)    \n",
    "\n",
    "model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6da485a8-c503-41c8-a680-87fc9de20070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_camembert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFCamembertMainLa  multiple                  110031360 \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " classifier (TFCamembertCla  multiple                  592130    \n",
      " ssificationHead)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110623490 (422.00 MB)\n",
      "Trainable params: 110623490 (422.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initial_weights = model.get_weights()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e729a5c6-f4b9-422a-98c2-e3ec561020da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 14035s 3s/step - loss: 0.1730 - accuracy: 0.9373 - val_loss: 0.0971 - val_accuracy: 0.9643\n"
     ]
    }
   ],
   "source": [
    "# TODO (in fact, done in the Accuracy vs Training Data part)\n",
    "history = model.fit(\n",
    "    encoded_train, y_train, epochs=1, batch_size=4, \n",
    "    validation_data=(encoded_valid, y_val), verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d09ab770-2654-4906-a249-79e665158c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class EarlyStoppingModel(BaseEstimator):\n",
    "    def __init__(self, transformers_model, max_epoches, batch_size, validation_data):\n",
    "        self.model = transformers_model\n",
    "        self.max_epoches = max_epoches\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Defines early stopper\n",
    "        early_stopper = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', mode='auto', patience=2, # only 1 !\n",
    "            verbose=1, restore_best_weights=True\n",
    "        )        \n",
    "\n",
    "        # Train model on data subset\n",
    "        self.model.fit(\n",
    "            X, y,\n",
    "            validation_data=self.validation_data,\n",
    "            epochs=self.max_epoches, \n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[early_stopper], \n",
    "            verbose=1\n",
    "        )        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):        \n",
    "        scores = self.model.predict(X)\n",
    "        y_pred = np.argmax(scores, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5844a8a4-b76b-487d-97fc-9793903a7028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "\n",
    "def accuracy_vs_training_data(camembert_model, initial_weights, \n",
    "                              preprocessor, sizes,\n",
    "                              train_reviews, train_labels,\n",
    "                              val_reviews, val_labels,\n",
    "                              test_reviews, test_labels):\n",
    "    test_accuracies = []\n",
    "    for size in sizes:        \n",
    "        # Preprocess data\n",
    "        X_train, y_train = preprocessor.fit_transform(\n",
    "            train_reviews[:size], train_labels[:size]\n",
    "        )\n",
    "        X_val, y_val = preprocessor.transform(val_reviews, val_labels)\n",
    "        X_test, y_test = preprocessor.transform(test_reviews, test_labels)\n",
    "        \n",
    "        # Reset weights to initial value\n",
    "        camembert_model.set_weights(initial_weights)\n",
    "        best_model = EarlyStoppingModel(\n",
    "            camembert_model, max_epoches=20, batch_size=4,\n",
    "            validation_data=(X_val, y_val)\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        test_acc = metrics.accuracy_score(y_test, y_pred)\n",
    "        test_accuracies.append(test_acc)\n",
    "        print(\"Test acc: \" + str(test_acc))\n",
    "        \n",
    "    return test_accuracies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6723d-8ced-4751-8e2b-46995c2283ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 1575s 13s/step - loss: 0.6867 - accuracy: 0.5720 - val_loss: 0.6845 - val_accuracy: 0.5243\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1574s 13s/step - loss: 0.6807 - accuracy: 0.5680 - val_loss: 0.6740 - val_accuracy: 0.5299\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1577s 13s/step - loss: 0.6632 - accuracy: 0.6120 - val_loss: 0.6413 - val_accuracy: 0.7095\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 1577s 13s/step - loss: 0.5988 - accuracy: 0.8120 - val_loss: 0.5130 - val_accuracy: 0.8849\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 1576s 13s/step - loss: 0.4271 - accuracy: 0.9180 - val_loss: 0.2916 - val_accuracy: 0.9256\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 1574s 13s/step - loss: 0.2515 - accuracy: 0.9420 - val_loss: 0.2110 - val_accuracy: 0.9355\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1575s 13s/step - loss: 0.1692 - accuracy: 0.9660 - val_loss: 0.1835 - val_accuracy: 0.9411\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 1576s 13s/step - loss: 0.1123 - accuracy: 0.9800 - val_loss: 0.1750 - val_accuracy: 0.9441\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 1574s 13s/step - loss: 0.0740 - accuracy: 0.9880 - val_loss: 0.1731 - val_accuracy: 0.9463\n",
      "Epoch 10/20\n",
      " 28/125 [=====>........................] - ETA: 4:07 - loss: 0.0804 - accuracy: 0.9821"
     ]
    }
   ],
   "source": [
    "sizes = [int(p) for p in np.geomspace(500, 160000, 5)]\n",
    "preprocessor = CamembertPreprocessor(tokenizer, MAX_SEQ_LEN)\n",
    "\n",
    "test_accuracies = accuracy_vs_training_data(\n",
    "    model, initial_weights, \n",
    "    preprocessor, sizes,\n",
    "    train_reviews, train_labels,\n",
    "    validation_reviews, validation_labels,\n",
    "    test_reviews, test_labels\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75720c2-f278-4d66-b545-de3789e96847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving last model (full dataset)\n",
    "model.save_weights('data/bert/camembert_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d8fe2e-a98d-48da-9e85-dce0cfee4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error analysis\n",
    "from sklearn import metrics\n",
    "\n",
    "model.load_weights('data/camembert_weights.hdf5')\n",
    "scores = model.predict(encoded_valid)\n",
    "y_pred = np.argmax(scores, axis=1)\n",
    "    \n",
    "print(\"Val Accuracy: {:.2f}\".format(100 * metrics.accuracy_score(y_val, y_pred)))\n",
    "print(\"Val F1-Score: {:.2f}\".format(100 * metrics.f1_score(y_val, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6e418-b2e9-47a7-8f38-879b7c21e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix                       #confusion matrix\n",
    "\n",
    "conf_mx = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "fig = print_confusion_matrix(\n",
    "    conf_mx, \n",
    "    class_names.values(), \n",
    "    figsize=(7,5)\n",
    ")\n",
    "\n",
    "# Finalize the plot\n",
    "sns.despine(bottom=True)\n",
    "plt.tight_layout(h_pad=2)\n",
    "\n",
    "# Saving plot\n",
    "fig.savefig('img/bert/val_confusion_mx.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d7338-5884-4a9c-b4bd-c1033078321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## False positive / negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf02a50-3532-4355-be34-3e69cd2ee268",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos = val_reviews[(y_val == 0) & (y_pred == 1)]\n",
    "false_neg = val_reviews[(y_val == 1) & (y_pred == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196def4-e97d-48c3-bd42-e3f0cdd466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(false_pos[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e90b1-12f3-4c96-9d2c-3a8d04550096",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(false_neg[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87d4d5-681b-41e2-a67f-2b96a267d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training best model\n",
    "model.load_weights('data/bert/camembert_weights.hdf5')\n",
    "\n",
    "scores = model.predict(encoded_test)\n",
    "y_pred = np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca6a97-2c2f-4d3e-87f2-dd1927d4fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "    \n",
    "print(\"Test Accuracy: {:.2f}\".format(100 * metrics.accuracy_score(y_test, y_pred)))\n",
    "print(\"Test F1-Score: {:.2f}\".format(100 * metrics.f1_score(y_test, y_pred)))\n",
    "print()\n",
    "\n",
    "report = metrics.classification_report(\n",
    "    y_test, y_pred, \n",
    "    target_names=class_names.values()\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57b5b0-6a86-4b89-b0aa-e8a2226e69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mx = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig = print_confusion_matrix(\n",
    "    conf_mx, \n",
    "    class_names.values(), \n",
    "    figsize=(7,5)\n",
    ")\n",
    "\n",
    "# Finalize the plot\n",
    "sns.despine(bottom=True)\n",
    "plt.tight_layout(h_pad=2)\n",
    "\n",
    "# Saving plot\n",
    "fig.savefig('img/bert/test_confusion_mx.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90334a82-82a2-4331-b6be-5080f2f98143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c4ade5-6528-4e0f-9df0-a246b9ad00f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
